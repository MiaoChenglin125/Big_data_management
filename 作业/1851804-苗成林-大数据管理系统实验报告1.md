> ##### 同济大学脑认知与智能计算课程报告01
>
> 



#                               《Transformer在视觉领域的应用》



> - 学院：电子信息工程学院

> - 专业：数据科学与大数据专业
> - 学号：1851804
> - 姓名：苗成林
> - 指导教师：孙杳如
> - 时间：2021.10.18





Transformer 是 Google 的团队在 2017 年提出的一种 NLP 经典模型，现在比较火热的 Bert 也是基于 Transformer。Transformer 模型使用了 Self-Attention 机制，**不采用** RNN 的**顺序结构**，使得模型**可以并行化训练**，而且能够**拥有全局信息。**本文介绍的是Transformer的基础，包含什么是Attention，什么是Transformer，以及在检测上的应用。其实在DETR之前已经有人尝试过把self-attention机制或者Transformer应用在视觉任务上面，但是关注不多。DETR的出现使得这个模型开始广泛应用在各种视觉任务上面。



## 1 Self-attention

- **1.1 处理Sequence数据的模型：**

Transformer是一个Sequence to Sequence model，特别之处在于它大量用到了self-attention。

要处理一个Sequence，最常想到的就是使用RNN，它的输入是一串vector sequence，输出是另一串vector sequence，如下图1左所示。

如果假设是一个single directional的RNN，那当输出 ![[公式]](https://www.zhihu.com/equation?tex=b_4) 时，默认 ![[公式]](https://www.zhihu.com/equation?tex=a_1%2Ca_2%2Ca_3%2Ca_4) 都已经看过了。如果假设是一个bi-directional的RNN，那当输出 ![[公式]](https://www.zhihu.com/equation?tex=b_{任意}) 时，默认 ![[公式]](https://www.zhihu.com/equation?tex=a_1%2Ca_2%2Ca_3%2Ca_4) 都已经看过了。RNN非常擅长于处理input是一个sequence的状况。

RNN的问题就在于：RNN很不容易并行化 (hard to parallel)。

RNN很不容易并行化,，假设在single directional的RNN的情形下，你要算出 ![[公式]](https://www.zhihu.com/equation?tex=b_4) ，就必须要先看 ![[公式]](https://www.zhihu.com/equation?tex=a_1) 再看 ![[公式]](https://www.zhihu.com/equation?tex=a_2) 再看 ![[公式]](https://www.zhihu.com/equation?tex=a_3) 再看 ![[公式]](https://www.zhihu.com/equation?tex=a_4) ，所以这个过程很难平行处理。

所以有人提出把CNN拿来取代RNN，如下图1右所示。其中，橘色的三角形表示一个filter，每次扫过3个向量 ![[公式]](https://www.zhihu.com/equation?tex=a) ，扫过一轮以后，就输出了一排结果，使用橘色的小圆点表示。

这是第一个橘色的filter的过程，还有其他的filter，比如图2中的黄色的filter，它经历着与橘色的filter相似的过程，又输出一排结果，使用黄色的小圆点表示。

<img src="https://i.loli.net/2021/11/14/KAorqv8wnIJzRCN.jpg" alt="img" style="zoom: 50%;" />

图1：处理Sequence数据的模型

<img src="https://i.loli.net/2021/11/14/D4vljczTNhB2LHr.jpg" alt="img" style="zoom: 25%;" />

图2：处理Sequence数据的模型



所以用CNN确实也可以做到跟RNN的输入输出类似的关系，也可以做到输入是一个sequence，输出是另外一个sequence。

但是，表面上CNN和RNN可以做到相同的输入和输出，但是CNN只能考虑非常有限的内容。比如在我们右侧的图中CNN的filter只考虑了3个vector，不像RNN可以考虑之前的所有vector。但是CNN也不是没有办法考虑很长时间的dependency的，你只需要堆叠filter，多堆叠几层，上层的filter就可以考虑比较多的资讯，比如，第二层的filter (蓝色的三角形)看了6个vector，所以，只要叠很多层，就能够看很长时间的资讯。

而CNN的一个好处是：它是可以并行化的 (can parallel)，不需要等待红色的filter算完，再算黄色的filter。但是必须要叠很多层filter，才可以看到长时的资讯。所以今天有一个想法：self-attention，如下图3所示，目的是使用self-attention layer取代RNN所做的事情。

<img src="https://i.loli.net/2021/11/14/bAMFGuxeDaozUsH.jpg" alt="img" style="zoom: 25%;" />

图3：You can try to replace any thing that has been done by RNNwith self attention



**所以重点是有一种新的layer，叫self-attention，它的输入和输出和RNN是一模一样的，输入一个sequence，输出一个sequence，它的每一个输出 ![[公式]](https://www.zhihu.com/equation?tex=b_1-b_4) 都看过了整个的输入sequence，这一点与bi-directional RNN相同。但是神奇的地方是：它的每一个输出 ![[公式]](https://www.zhihu.com/equation?tex=b_1-b_4)可以并行化计算。**



- **1.2 Self-attention：**

<img src="https://i.loli.net/2021/11/14/5iOJv2rRqlUBVhC.jpg" alt="img" style="zoom: 50%;" />

图4：self-attention具体是怎么做的？



首先假设我们的input是图4的 ![[公式]](https://www.zhihu.com/equation?tex=x_1-x_4) ，是一个sequence，每一个input (vector)先乘上一个矩阵 ![[公式]](https://www.zhihu.com/equation?tex=W) 得到embedding，即向量 ![[公式]](https://www.zhihu.com/equation?tex=a_1-a_4) 。接着这个embedding进入self-attention层，每一个向量 ![[公式]](https://www.zhihu.com/equation?tex=a_1-a_4) 分别乘上3个不同的transformation matrix ![[公式]](https://www.zhihu.com/equation?tex=W_q%2CW_k%2CW_v) ，以向量 ![[公式]](https://www.zhihu.com/equation?tex=a_1) 为例，分别得到3个不同的向量 ![[公式]](https://www.zhihu.com/equation?tex=q_1%2Ck_1%2Cv_1) 。

<img src="https://i.loli.net/2021/11/14/JuVhRaNYdrI45ej.jpg" alt="img" style="zoom: 50%;" />

图5：self-attention具体是怎么做的？



接下来使用每个query ![[公式]](https://www.zhihu.com/equation?tex=q) 去对每个key ![[公式]](https://www.zhihu.com/equation?tex=k) 做attention，attention就是匹配这2个向量有多接近，比如我现在要对 ![[公式]](https://www.zhihu.com/equation?tex=q^1) 和 ![[公式]](https://www.zhihu.com/equation?tex=k^1) 做attention，我就可以把这2个向量做**scaled inner product**，得到 ![[公式]](https://www.zhihu.com/equation?tex=\alpha_{1%2C1}) 。接下来你再拿 ![[公式]](https://www.zhihu.com/equation?tex=q%5E1) 和 ![[公式]](https://www.zhihu.com/equation?tex=k^2) 做attention，得到 ![[公式]](https://www.zhihu.com/equation?tex=\alpha_{1%2C2}) ，你再拿 ![[公式]](https://www.zhihu.com/equation?tex=q%5E1) 和 ![[公式]](https://www.zhihu.com/equation?tex=k^3) 做attention，得到 ![[公式]](https://www.zhihu.com/equation?tex=\alpha_{1%2C3}) ，你再拿 ![[公式]](https://www.zhihu.com/equation?tex=q%5E1) 和 ![[公式]](https://www.zhihu.com/equation?tex=k^4) 做attention，得到 ![[公式]](https://www.zhihu.com/equation?tex=\alpha_{1%2C4}) 。scaled inner product的具体计算。

![[公式]](https://www.zhihu.com/equation?tex=\alpha_{1%2Ci}%3Dq^1\cdot+k^i%2F\sqrt{d}+\tag{1}) 

式中， ![[公式]](https://www.zhihu.com/equation?tex=d) 是 ![[公式]](https://www.zhihu.com/equation?tex=q) 跟 ![[公式]](https://www.zhihu.com/equation?tex=k) 的维度。因为 ![[公式]](https://www.zhihu.com/equation?tex=q\cdot+k) 的数值会随着dimension的增大而增大，所以要除以 ![[公式]](https://www.zhihu.com/equation?tex=\sqrt{\text{dimension}}) 的值，相当于归一化的效果。



接下来要做的事如图6所示，把计算得到的所有 ![[公式]](https://www.zhihu.com/equation?tex=\alpha_{1%2Ci}) 值取 ![[公式]](https://www.zhihu.com/equation?tex=\text{softmax}) 操作。

<img src="https://pic2.zhimg.com/v2-58f7bf32a29535b57205ac2dab557be1_b.jpg" alt="img" style="zoom: 50%;" />

图6：self-attention具体是怎么做的？



取完 ![[公式]](https://www.zhihu.com/equation?tex=\text{softmax}) 操作以后，我们得到了 ![[公式]](https://www.zhihu.com/equation?tex=\hat+\alpha_{1%2Ci}) ，我们用它和所有的 ![[公式]](https://www.zhihu.com/equation?tex=v^i) 值进行相乘。具体来讲，把 ![[公式]](https://www.zhihu.com/equation?tex=\hat+\alpha_{1%2C1}) 乘上 ![[公式]](https://www.zhihu.com/equation?tex=v^1) ，把 ![[公式]](https://www.zhihu.com/equation?tex=\hat+\alpha_{1%2C2}) 乘上 ![[公式]](https://www.zhihu.com/equation?tex=v^2) ，把 ![[公式]](https://www.zhihu.com/equation?tex=\hat+\alpha_{1%2C3}) 乘上 ![[公式]](https://www.zhihu.com/equation?tex=v^3) ，把 ![[公式]](https://www.zhihu.com/equation?tex=\hat+\alpha_{1%2C4}) 乘上 ![[公式]](https://www.zhihu.com/equation?tex=v^4) ，把结果通通加起来得到 ![[公式]](https://www.zhihu.com/equation?tex=b^1) ，所以，今天在产生 ![[公式]](https://www.zhihu.com/equation?tex=b%5E1) 的过程中用了整个sequence的资讯 (Considering the whole sequence)。如果要考虑local的information，则只需要学习出相应的 ![[公式]](https://www.zhihu.com/equation?tex=\hat+\alpha_{1%2Ci}%3D0) ， ![[公式]](https://www.zhihu.com/equation?tex=b%5E1) 就不再带有那个对应分支的信息了；如果要考虑global的information，则只需要学习出相应的 ![[公式]](https://www.zhihu.com/equation?tex=\hat+\alpha_{1%2Ci}\ne0) ， ![[公式]](https://www.zhihu.com/equation?tex=b%5E1) 就带有全部的对应分支的信息了。

<img src="https://pic3.zhimg.com/v2-b7e1ffade85d4dbe3350f23e6854c272_b.jpg" alt="img" style="zoom: 25%;" />

图7：self-attention具体是怎么做的？



同样的方法，也可以计算出 ![[公式]](https://www.zhihu.com/equation?tex=b^2%2Cb^3%2Cb^4) ，如下图8所示， ![[公式]](https://www.zhihu.com/equation?tex=b^2) 就是拿query  ![[公式]](https://www.zhihu.com/equation?tex=q^2)去对其他的 ![[公式]](https://www.zhihu.com/equation?tex=k) 做attention，得到 ![[公式]](https://www.zhihu.com/equation?tex=\hat+\alpha_{2%2Ci}) ，再与value值 ![[公式]](https://www.zhihu.com/equation?tex=v^i) 相乘取weighted sum得到的。

<img src="https://pic2.zhimg.com/v2-f7b03e1979c6ccd1dab4b579654c8cd5_b.jpg" alt="img" style="zoom: 25%;" />

图8：self-attention具体是怎么做的？



经过了以上一连串计算，self-attention layer做的事情跟RNN是一样的，只是它可以并行的得到layer输出的结果，如图9所示。现在我们要用矩阵表示上述的计算过程。

<img src="https://pic2.zhimg.com/v2-67bc90b683b40488e922dcd5abcaa089_b.jpg" alt="img" style="zoom: 25%;" />

图9：self-attention的效果



首先输入的embedding是 ![[公式]](https://www.zhihu.com/equation?tex=I%3D[a^1%2Ca^2%2Ca^3%2Ca^4]) ，然后用 ![[公式]](https://www.zhihu.com/equation?tex=I) 乘以transformation matrix ![[公式]](https://www.zhihu.com/equation?tex=W^q) 得到 ![[公式]](https://www.zhihu.com/equation?tex=Q%3D[q^1%2Cq^2%2Cq^3%2Cq^4]) ，它的每一列代表着一个vector ![[公式]](https://www.zhihu.com/equation?tex=q) 。同理，用 ![[公式]](https://www.zhihu.com/equation?tex=I) 乘以transformation matrix ![[公式]](https://www.zhihu.com/equation?tex=W^k) 得到 ![[公式]](https://www.zhihu.com/equation?tex=K%3D[k^1%2Ck^2%2Ck^3%2Ck^4]) ，它的每一列代表着一个vector ![[公式]](https://www.zhihu.com/equation?tex=k) 。用 ![[公式]](https://www.zhihu.com/equation?tex=I) 乘以transformation matrix ![[公式]](https://www.zhihu.com/equation?tex=W^v) 得到 ![[公式]](https://www.zhihu.com/equation?tex=Q%3D[v^1%2Cv^2%2Cv^3%2Cv^4]) ，它的每一列代表着一个vector ![[公式]](https://www.zhihu.com/equation?tex=v) 。

<img src="https://pic2.zhimg.com/v2-b081f7cbc5ecd2471567426e696bde15_b.jpg" alt="img" style="zoom: 25%;" />

图10：self-attention的矩阵计算过程



接下来是 ![[公式]](https://www.zhihu.com/equation?tex=k) 与 ![[公式]](https://www.zhihu.com/equation?tex=q) 的attention过程，我们可以把vector ![[公式]](https://www.zhihu.com/equation?tex=k) 横过来变成行向量，与列向量 ![[公式]](https://www.zhihu.com/equation?tex=q) 做内积，这里省略了 ![[公式]](https://www.zhihu.com/equation?tex=\sqrt{d}) 。这样， ![[公式]](https://www.zhihu.com/equation?tex=\alpha) 就成为了 ![[公式]](https://www.zhihu.com/equation?tex=4\times4) 的矩阵，它由4个行向量拼成的矩阵和4个列向量拼成的矩阵做内积得到，如图11所示。

在得到 ![[公式]](https://www.zhihu.com/equation?tex=\hat+A) 以后，如上文所述，要得到 ![[公式]](https://www.zhihu.com/equation?tex=b%5E1)， 就要使用 ![[公式]](https://www.zhihu.com/equation?tex=\hat+\alpha_{1%2Ci}) 分别与 ![[公式]](https://www.zhihu.com/equation?tex=v^i) 相乘再求和得到，所以 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat+A) 要再左乘 ![[公式]](https://www.zhihu.com/equation?tex=V) 矩阵。

<img src="https://pic3.zhimg.com/v2-6cc342a83d25ac76b767b5bbf27d9d6e_b.jpg" alt="img" style="zoom: 25%;" />

<img src="https://pic2.zhimg.com/v2-52a5e6b928dc44db73f85001b2d1133d_b.jpg" alt="img" style="zoom: 50%;" />

<img src="https://pic4.zhimg.com/v2-1b7d30f098f02488c48c3601f8e13033_b.jpg" alt="img" style="zoom: 50%;" />

图11：self-attention的矩阵计算过程



到这里你会发现这个过程可以被表示为，如图12所示：输入矩阵 ![[公式]](https://www.zhihu.com/equation?tex=I\in+R+(d%2CN)) 分别乘上3个不同的矩阵 ![[公式]](https://www.zhihu.com/equation?tex=W_q%2CW_k%2CW_v+\in+R+(d%2Cd)) 得到3个中间矩阵 ![[公式]](https://www.zhihu.com/equation?tex=Q%2CK%2CV\in+R+(d%2CN)) 。它们的维度是相同的。把 ![[公式]](https://www.zhihu.com/equation?tex=K) 转置之后与 ![[公式]](https://www.zhihu.com/equation?tex=Q) 相乘得到Attention矩阵 ![[公式]](https://www.zhihu.com/equation?tex=A\in+R+(N%2CN)) ，代表每一个位置两两之间的attention。再将它取 ![[公式]](https://www.zhihu.com/equation?tex=\text{softmax}) 操作得到 ![[公式]](https://www.zhihu.com/equation?tex=\hat+A\in+R+(N%2CN)) ，最后将它乘以 ![[公式]](https://www.zhihu.com/equation?tex=V) 矩阵得到输出vector ![[公式]](https://www.zhihu.com/equation?tex=O\in+R+(d%2CN)) 。

![[公式]](https://www.zhihu.com/equation?tex=\hat+A%3D\text{softmax}(A)%3DK^T\cdot+Q+\tag{2}) 

![[公式]](https://www.zhihu.com/equation?tex=O%3DV\cdot\hat+A\tag{3}) 

<img src="https://i.loli.net/2021/11/14/75NmOlRevKi4qZ8.jpg" alt="img" style="zoom: 50%;" />

图12：self-attention就是一堆矩阵乘法，可以实现GPU加速



- **1.3 Multi-head Self-attention：**

还有一种multi-head的self-attention，以2个head的情况为例：由 ![[公式]](https://www.zhihu.com/equation?tex=a^i) 生成的 ![[公式]](https://www.zhihu.com/equation?tex=q^i) 进一步乘以2个转移矩阵变为 ![[公式]](https://www.zhihu.com/equation?tex=q^{i%2C1}) 和 ![[公式]](https://www.zhihu.com/equation?tex=q^{i%2C2}) ，同理由 ![[公式]](https://www.zhihu.com/equation?tex=a%5Ei) 生成的 ![[公式]](https://www.zhihu.com/equation?tex=k^i) 进一步乘以2个转移矩阵变为 ![[公式]](https://www.zhihu.com/equation?tex=k^{i%2C1}) 和 ![[公式]](https://www.zhihu.com/equation?tex=k^{i%2C2}) ，由 ![[公式]](https://www.zhihu.com/equation?tex=a%5Ei) 生成的 ![[公式]](https://www.zhihu.com/equation?tex=v^i) 进一步乘以2个转移矩阵变为 ![[公式]](https://www.zhihu.com/equation?tex=v^{i%2C1}) 和 ![[公式]](https://www.zhihu.com/equation?tex=v^{i%2C2}) 。接下来 ![[公式]](https://www.zhihu.com/equation?tex=q%5E%7Bi%2C1%7D) 再与 ![[公式]](https://www.zhihu.com/equation?tex=k%5E%7Bi%2C1%7D) 做attention，得到weighted sum的权重 ![[公式]](https://www.zhihu.com/equation?tex=\alpha) ，再与 ![[公式]](https://www.zhihu.com/equation?tex=v%5E%7Bi%2C1%7D) 做weighted sum得到最终的 ![[公式]](https://www.zhihu.com/equation?tex=b^{i%2C1}(i%3D1%2C2%2C...%2CN)) 。同理得到 ![[公式]](https://www.zhihu.com/equation?tex=b^{i%2C2}(i%3D1%2C2%2C...%2CN)) 。现在我们有了 ![[公式]](https://www.zhihu.com/equation?tex=b^{i%2C1}(i%3D1%2C2%2C...%2CN)\in+R(d%2C1)) 和 ![[公式]](https://www.zhihu.com/equation?tex=b^{i%2C2}(i%3D1%2C2%2C...%2CN)\in+R(d%2C1)) ，可以把它们concat起来，再通过一个transformation matrix调整维度，使之与刚才的 ![[公式]](https://www.zhihu.com/equation?tex=b^{i}(i%3D1%2C2%2C...%2CN)\in+R(d%2C1)) 维度一致(这步如图13所示)。

<img src="https://pic1.zhimg.com/v2-688516477ad57f01a4abe5fd1a36e510_b.jpg" alt="img" style="zoom: 50%;" />

<img src="https://pic3.zhimg.com/v2-b0891e9352874c9eee469372b85ecbe2_b.jpg" alt="img" style="zoom: 25%;" />

图13：multi-head self-attention

<img src="https://pic1.zhimg.com/v2-df5d332304c2fd217705f210edd18bf4_b.jpg" alt="img" style="zoom: 25%;" />

图13：调整b的维度



从下图14可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入 ![[公式]](https://www.zhihu.com/equation?tex=X) 分别传递到 2个不同的 Self-Attention 中，计算得到 2 个输出结果。得到2个输出矩阵之后，Multi-Head Attention 将它们拼接在一起 (Concat)，然后传入一个Linear层，得到 Multi-Head Attention 最终的输出 ![[公式]](https://www.zhihu.com/equation?tex=Z) 。可以看到 Multi-Head Attention 输出的矩阵 ![[公式]](https://www.zhihu.com/equation?tex=Z) 与其输入的矩阵 ![[公式]](https://www.zhihu.com/equation?tex=X) 的维度是一样的。

<img src="https://pic2.zhimg.com/v2-f784c73ae6eb34a00108b64e3db394fd_b.jpg" alt="img" style="zoom: 50%;" />

图14：multi-head self-attention



这里有一组Multi-head Self-attention的解果，其中绿色部分是一组query和key，红色部分是另外一组query和key，可以发现绿色部分其实更关注global的信息，而红色部分其实更关注local的信息。

<img src="https://pic3.zhimg.com/v2-6b6c906cfca399506d324cac3292b04a_b.jpg" alt="img" style="zoom: 33%;" />

图15：Multi-head Self-attention的不同head分别关注了global和local的讯息



- **1.4 Positional Encoding：**

以上是multi-head self-attention的原理，但是还有一个问题是：现在的self-attention中没有位置的信息，一个单词向量的“近在咫尺”位置的单词向量和“远在天涯”位置的单词向量效果是一样的，没有表示位置的信息(No position information in self attention)。所以你输入"A打了B"或者"B打了A"的效果其实是一样的，因为并没有考虑位置的信息。所以在self-attention原来的paper中，作者为了解决这个问题所做的事情是如下图16所示：

<img src="https://pic3.zhimg.com/v2-b8886621fc841085300f5bb21de26f0e_b.jpg" alt="img" style="zoom: 25%;" />

<img src="https://pic4.zhimg.com/v2-7814595d02ef37cb762b3ef998fae267_b.jpg" alt="img" style="zoom: 33%;" />

图16：self-attention中的位置编码



具体的做法是：给每一个位置规定一个表示位置信息的向量 ![[公式]](https://www.zhihu.com/equation?tex=e^i) ，让它与 ![[公式]](https://www.zhihu.com/equation?tex=a%5Ei) 加在一起之后作为新的 ![[公式]](https://www.zhihu.com/equation?tex=a%5Ei) 参与后面的运算过程，但是这个向量 ![[公式]](https://www.zhihu.com/equation?tex=e%5Ei) 是由人工设定的，而不是神经网络学习出来的。每一个位置都有一个不同的 ![[公式]](https://www.zhihu.com/equation?tex=e%5Ei) 。

那到这里一个自然而然的问题是：**为什么是 ![[公式]](https://www.zhihu.com/equation?tex=e%5Ei) 与 ![[公式]](https://www.zhihu.com/equation?tex=a%5Ei) 相加？为什么不是concatenate？加起来以后，原来表示位置的资讯不就混到 ![[公式]](https://www.zhihu.com/equation?tex=a%5Ei) 里面去了吗？不就很难被找到了吗？**

**这里提供一种解答这个问题的思路：**

如图15所示，我们先给每一个位置的 ![[公式]](https://www.zhihu.com/equation?tex=x^i\in+R(d%2C1)) append一个one-hot编码的向量 ![[公式]](https://www.zhihu.com/equation?tex=p^i\in+R(N%2C1)) ，得到一个新的输入向量 ![[公式]](https://www.zhihu.com/equation?tex=x_p^i\in+R(d%2BN%2C1)) ，这个向量作为新的输入，乘以一个transformation matrix ![[公式]](https://www.zhihu.com/equation?tex=W%3D[W^I%2CW^P]\in+R(d%2Cd%2BN)) 。那么：

![[公式]](https://www.zhihu.com/equation?tex=W\cdot+x_p^i%3D[W^I%2CW^P]\cdot\begin{bmatrix}x^i\\p^i+\end{bmatrix}%3DW^I\cdot+x^i%2BW^P\cdot+p^i%3Da^i%2Be^i+\tag{4}) 

**所以，![[公式]](https://www.zhihu.com/equation?tex=e%5Ei) 与 ![[公式]](https://www.zhihu.com/equation?tex=a%5Ei) 相加就等同于把原来的输入 ![[公式]](https://www.zhihu.com/equation?tex=x^i) concat一个表示位置的独热编码 ![[公式]](https://www.zhihu.com/equation?tex=p^i) ，再做transformation。**

**这个与位置编码乘起来的矩阵** ![[公式]](https://www.zhihu.com/equation?tex=W^P) 是手工设计的，如图17所示 (黑色框代表一个位置的编码)。

<img src="https://pic4.zhimg.com/v2-8b7cf3525520292bdfa159463d9717db_b.jpg" alt="img" style="zoom: 33%;" />

图17：与位置编码乘起来的转移矩阵WP



Transformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。

位置 Embedding 用 PE表示，PE 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：



![[公式]](https://www.zhihu.com/equation?tex=\begin{align}PE_{(pos%2C+2i)}+%3D+sin(pos%2F10000^{2i%2Fd_{model}})+\\+PE_{(pos%2C+2i%2B1)}+%3D+cos(pos%2F10000^{2i%2Fd_{model}})++\end{align}\tag{5})

式中， ![[公式]](https://www.zhihu.com/equation?tex=pos) 表示token在sequence中的位置，例如第一个token "我" 的 ![[公式]](https://www.zhihu.com/equation?tex=pos%3D0) 。

![[公式]](https://www.zhihu.com/equation?tex=i) ，或者准确意义上是 ![[公式]](https://www.zhihu.com/equation?tex=2i) 和 ![[公式]](https://www.zhihu.com/equation?tex=2i%2B1) 表示了Positional Encoding的维度，![[公式]](https://www.zhihu.com/equation?tex=i) 的取值范围是：。所以当 ![[公式]](https://www.zhihu.com/equation?tex=pos) 为1时，对应的Positional Encoding可以写成：

![[公式]](https://www.zhihu.com/equation?tex=PE\left(+1+\right)%3D\left[+\sin+\left(+{1}%2F{{{10000}^{{0}%2F{512}\%3B}}}\%3B+\right)%2C\cos+\left(+{1}%2F{{{10000}^{{0}%2F{512}\%3B}}}\%3B+\right)%2C\sin+\left(+{1}%2F{{{10000}^{{2}%2F{512}\%3B}}}\%3B+\right)%2C\cos+\left(+{1}%2F{{{10000}^{{2}%2F{512}\%3B}}}\%3B+\right)%2C\ldots++\right])

式中， ![[公式]](https://www.zhihu.com/equation?tex={{d}_{model}}%3D512)。底数是10000。为什么要使用10000呢，这个就类似于玄学了，原论文中完全没有提啊，这里不得不说说论文的readability的问题，即便是很多高引的文章，最基本的内容都讨论不清楚，所以才出现像上面提问里的讨论，说实话这些论文还远远没有做到easy to follow。这里我给出一个假想：![[公式]](https://www.zhihu.com/equation?tex={{10000}^{{1}%2F{512}}})是一个比较接近1的数（1.018），如果用100000，则是1.023。这里只是猜想一下，其实大家应该完全可以使用另一个底数。



这个式子的好处是：

- 每个位置有一个唯一的positional encoding。
- 使 ![[公式]](https://www.zhihu.com/equation?tex=PE) 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。
- 可以让模型容易地计算出相对位置，对于固定长度的间距 ![[公式]](https://www.zhihu.com/equation?tex=k) ，任意位置的 ![[公式]](https://www.zhihu.com/equation?tex=PE_{pos%2Bk}) 都可以被 ![[公式]](https://www.zhihu.com/equation?tex=PE_{pos}) 的线性函数表示，因为三角函数特性：

![[公式]](https://www.zhihu.com/equation?tex=cos(\alpha%2B\beta)+%3D+cos(\alpha)cos(\beta)-sin(\alpha)sin(\beta)+\\)

![[公式]](https://www.zhihu.com/equation?tex=sin(\alpha%2B\beta)+%3D+sin(\alpha)cos(\beta)+%2B+cos(\alpha)sins(\beta)+\\)

除了以上的固定位置编码以外，还有其他的很多表示方法：

比如下图18a就是sin-cos的固定位置编码。图b就是可学习的位置编码。图c和d分别FLOATER和RNN模型学习的位置编码。

<img src="https://pic3.zhimg.com/v2-4ef2648c2bebe2621c0c03001c0e1b92_b.jpg" alt="img" style="zoom: 50%;" />

图18：其他的很多位置编码方法



接下来我们看看self-attention在sequence2sequence model里面是怎么使用的，我们可以把Encoder-Decoder中的RNN用self-attention取代掉。

<img src="https://pic4.zhimg.com/v2-287ebca58558012f9459f3f1d5bc3827_b.jpg" alt="img" style="zoom: 50%;" />

图18：Seq2seq with Attention



在self-attention的最后一部分我们来对比下self-attention和CNN的关系。如图19，今天在使用self-attention去处理一张图片的时候，1的那个pixel产生query，其他的各个pixel产生key。在做inner-product的时候，考虑的不是一个小的范围，而是一整张图片。

但是在做CNN的时候是只考虑感受野红框里面的资讯，而不是图片的全局信息。所以CNN可以看作是一种简化版本的self-attention。

或者可以反过来说，self-attention是一种复杂化的CNN，在做CNN的时候是只考虑感受野红框里面的资讯，而感受野的范围和大小是由人决定的。但是self-attention由attention找到相关的pixel，就好像是感受野的范围和大小是自动被学出来的，所以CNN可以看做是self-attention的特例，如图20所示。



<img src="https://i.loli.net/2021/11/14/uOBcH4ZU3ThKMdy.png" alt="img" style="zoom: 25%;" />

图19：CNN考虑感受野范围，而self-attention考虑的不是一个小的范围，而是一整张图片



<img src="https://pic4.zhimg.com/v2-f268035371aa22a350a317fc237a04f7_b.jpg" alt="img" style="zoom: 33%;" />

图20：CNN可以看做是self-attention的特例



既然self-attention是更广义的CNN，则这个模型更加flexible。而我们认为，一个模型越flexible，训练它所需要的数据量就越多，所以在训练self-attention模型时就需要更多的数据，这一点在下面介绍的论文 ViT 中有印证，它需要的数据集是有3亿张图片的JFT-300，而如果不使用这么多数据而只使用ImageNet，则性能不如CNN。



## 2 Transformer的实现

- **2.1 Transformer原理分析：**

<img src="https://pic4.zhimg.com/v2-1719966a223d98ad48f98c2e4d71add7_b.jpg" alt="img" style="zoom: 33%;" />

图21：Transformer

> **Encoder：**

这个图21讲的是一个seq2seq的model，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为Multi-Head Attention，是由多个Self-Attention组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add & Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。比如说在Encoder Input处的输入是机器学习，在Decoder Input处的输入是<BOS>，输出是machine。再下一个时刻在Decoder Input处的输入是machine，输出是learning。不断重复知道输出是句点(.)代表翻译结束。

接下来我们看看这个Encoder和Decoder里面分别都做了什么事情，先看左半部分的Encoder：首先输入 ![[公式]](https://www.zhihu.com/equation?tex=X\in+R+(n_x%2CN)) 通过一个Input Embedding的转移矩阵 ![[公式]](https://www.zhihu.com/equation?tex=W^X\in+R+(d%2Cn_x)) 变为了一个张量，即上文所述的 ![[公式]](https://www.zhihu.com/equation?tex=I\in+R+(d%2CN)) ，再加上一个表示位置的Positional Encoding ![[公式]](https://www.zhihu.com/equation?tex=E\in+R+(d%2CN)) ，得到一个张量，去往后面的操作。

它进入了这个绿色的block，这个绿色的block会重复 ![[公式]](https://www.zhihu.com/equation?tex=N) 次。这个绿色的block里面有什么呢？它的第1层是一个上文讲的multi-head的attention。你现在一个sequence ![[公式]](https://www.zhihu.com/equation?tex=I\in+R+(d%2CN)) ，经过一个multi-head的attention，你会得到另外一个sequence ![[公式]](https://www.zhihu.com/equation?tex=O\in+R+(d%2CN)) 。

下一个Layer是Add & Norm，这个意思是说：把multi-head的attention的layer的输入 ![[公式]](https://www.zhihu.com/equation?tex=I\in+R+(d%2CN)) 和输出 ![[公式]](https://www.zhihu.com/equation?tex=O\in+R+(d%2CN)) 进行相加以后，再做Layer Normalization，至于Layer Normalization和我们熟悉的Batch Normalization的区别是什么，请参考图20和21。




<img src="https://pic3.zhimg.com/v2-53267aa305030eb71376296a6fd14cde_b.jpg" alt="img" style="zoom:67%;" />

图22：不同Normalization方法的对比



其中，Batch Normalization和Layer Normalization的对比可以概括为图22，Batch Normalization强行让一个batch的数据的某个channel的 ![[公式]](https://www.zhihu.com/equation?tex=\mu%3D0%2C\sigma%3D1) ，而Layer Normalization让一个数据的所有channel的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu%3D0%2C%5Csigma%3D1) 。

<img src="https://pic1.zhimg.com/v2-4c13b36ec9a6a2d2f4911d2d9e7122b8_b.jpg" alt="img" style="zoom:50%;" />

图23：Batch Normalization和Layer Normalization的对比



接着是一个Feed Forward的前馈网络和一个Add & Norm Layer。

所以，这一个绿色的block的前2个Layer操作的表达式为：

![[公式]](https://www.zhihu.com/equation?tex=\color{darkgreen}{O_1}%3D\color{green}{\text{Layer+Normalization}}(\color{teal}{I}%2B\color{crimson}{\text{Multi-head+Self-Attention}}(\color{teal}{I}))) 

这一个绿色的block的后2个Layer操作的表达式为：

![[公式]](https://www.zhihu.com/equation?tex=\color{darkgreen}{O_2}%3D\color{green}{\text{Layer+Normalization}}(\color{teal}{O_1}%2B\color{crimson}{\text{Feed+Forward+Network}}(\color{teal}{O_1}))) 

![[公式]](https://www.zhihu.com/equation?tex=\color{green}{\text{Block}}(\color{teal}{I})%3D\color{green}{O_2}+\tag{8}) 

所以Transformer的Encoder的整体操作为：

![[公式]](https://www.zhihu.com/equation?tex=\color{purple}{\text{Encoder}}(\color{darkgreen}{I})%3D\color{darkgreen}{\text{Block}}(...\color{darkgreen}{\text{Block}}(\color{darkgreen}{\text{Block}})(\color{teal}{I}))) 



> **Decoder：**

现在来看Decoder的部分，输入包括2部分，下方是前一个time step的输出的embedding，即上文所述的 ![[公式]](https://www.zhihu.com/equation?tex=I\in+R+(d%2CN)) ，再加上一个表示位置的Positional Encoding ![[公式]](https://www.zhihu.com/equation?tex=E\in+R+(d%2CN)) ，得到一个张量，去往后面的操作。它进入了这个绿色的block，这个绿色的block会重复 ![[公式]](https://www.zhihu.com/equation?tex=N) 次。这个绿色的block里面有什么呢？

首先是Masked Multi-Head Self-attention，masked的意思是使attention只会attend on已经产生的sequence，这个很合理，因为还没有产生出来的东西不存在，就无法做attention。

**输出是：**对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{crimson}{i}) 位置的输出词的概率分布。

**输入是：** ![[公式]](https://www.zhihu.com/equation?tex=\color{purple}{Encoder}) **的输出** 和 **对应** ![[公式]](https://www.zhihu.com/equation?tex=\color{crimson}{i-1}) **位置decoder的输出**。所以中间的attention不是self-attention，它的Key和Value来自encoder，Query来自上一位置 ![[公式]](https://www.zhihu.com/equation?tex=\color{crimson}{Decoder}) 的输出。

**解码：这里要特别注意一下，编码可以并行计算，一次性全部Encoding出来，但解码不是一次把所有序列解出来的，而是像** ![[公式]](https://www.zhihu.com/equation?tex=RNN) **一样一个一个解出来的**，因为要用上一个位置的输入当作attention的query。

明确了解码过程之后最上面的图就很好懂了，这里主要的不同就是新加的另外要说一下新加的attention多加了一个mask，因为训练时的output都是Ground Truth，这样可以确保预测第 ![[公式]](https://www.zhihu.com/equation?tex=\color{crimson}{i}) 个位置时不会接触到未来的信息。

- 包含两个 Multi-Head Attention 层。
- 第一个 Multi-Head Attention 层采用了 Masked 操作。
- 第二个 Multi-Head Attention 层的Key，Value矩阵使用 Encoder 的编码信息矩阵 ![[公式]](https://www.zhihu.com/equation?tex=C) 进行计算，而Query使用上一个 Decoder block 的输出计算。
- 最后有一个 Softmax 层计算下一个翻译单词的概率。

下面详细介绍下Masked Multi-Head Self-attention的具体操作，**Masked在Scale操作之后，softmax操作之前**。

<img src="https://pic3.zhimg.com/v2-58ac6e864d336abce052cf36d480cfee_b.jpg" alt="img" style="zoom: 67%;" />

图24：Masked在Scale操作之后，softmax操作之前



因为在翻译的过程中是顺序翻译的，即翻译完第 ![[公式]](https://www.zhihu.com/equation?tex=i) 个单词，才可以翻译第 ![[公式]](https://www.zhihu.com/equation?tex=i%2B1) 个单词。通过 Masked 操作可以防止第 ![[公式]](https://www.zhihu.com/equation?tex=i) 个单词知道第 ![[公式]](https://www.zhihu.com/equation?tex=i%2B1) 个单词之后的信息。下面以 "我有一只猫" 翻译成 "I have a cat" 为例，了解一下 Masked 操作。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 "<Begin>" 预测出第一个单词为 "I"，然后根据输入 "<Begin> I" 预测下一个单词 "have"。

Decoder 可以在训练的过程中使用 Teacher Forcing **并且并行化训练，即将正确的单词序列 (<Begin> I have a cat) 和对应输出 (I have a cat <end>) 传递到 Decoder。那么在预测第** ![[公式]](https://www.zhihu.com/equation?tex=i) **个输出时，就要将第** ![[公式]](https://www.zhihu.com/equation?tex=i%2B1) **之后的单词掩盖住，**注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 "<Begin> I have a cat <end>"。

<img src="https://pic1.zhimg.com/v2-20d6a9f4b3cc8cbae05778816d1af414_b.jpg" alt="img" style="zoom:67%;" />

图25：Decoder过程



注意这里transformer模型训练和测试的方法不同：

**测试时：**

1. 输入<Begin>，解码器输出 I 。

2. 输入前面已经解码的<Begin>和 I，解码器输出have。

3. 输入已经解码的<Begin>，I, have, a, cat，解码器输出解码结束标志位<end>，每次解码都会利用前面已经解码输出的所有单词嵌入信息。

   

**Transformer测试时的解码过程：**

**训练时：**

**不采用上述类似RNN的方法**一个一个目标单词嵌入向量顺序输入训练，想采用**类似编码器中的矩阵并行算法，一步就把所有目标单词预测出来**。要实现这个功能就可以参考编码器的操作，把目标单词嵌入向量组成矩阵一次输入即可。即：**并行化训练。**

但是在解码have时候，不能利用到后面单词a和cat的目标单词嵌入向量信息，否则这就是作弊(测试时候不可能能未卜先知)。为此引入mask。具体是：在解码器中，self-attention层只被允许处理输出序列中更靠前的那些位置，在softmax步骤前，它会把后面的位置给隐去。



**Masked Multi-Head Self-attention的具体操作**如图26所示。

**Step1：**输入矩阵包含 "<Begin> I have a cat" (0, 1, 2, 3, 4) 五个单词的表示向量，Mask是一个 5×5 的矩阵。在Mask可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。输入矩阵 ![[公式]](https://www.zhihu.com/equation?tex=X\in+R_{N%2Cd_x}) 经过transformation matrix变为3个矩阵：Query ![[公式]](https://www.zhihu.com/equation?tex=Q\in+R_{N%2Cd}) ，Key ![[公式]](https://www.zhihu.com/equation?tex=K\in+R_{N%2Cd}) 和Value ![[公式]](https://www.zhihu.com/equation?tex=V\in+R_{N%2Cd}) 。



**Step2：** ![[公式]](https://www.zhihu.com/equation?tex=Q^T\cdot+K) 得到 Attention矩阵 ![[公式]](https://www.zhihu.com/equation?tex=A\in+R_{N%2CN}) ，此时先不急于做softmax的操作，而是先于一个 ![[公式]](https://www.zhihu.com/equation?tex=\text{Mask}\in+R_{N%2CN}) 矩阵相乘，使得attention矩阵的有些位置 归0，得到Masked Attention矩阵 ![[公式]](https://www.zhihu.com/equation?tex=\text{Mask+Attention}\in+R_{N%2CN}) 。 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BMask%7D%5Cin+R_%7BN%2CN%7D) 矩阵是个下三角矩阵，为什么这样设计？是因为想在计算 ![[公式]](https://www.zhihu.com/equation?tex=Z) 矩阵的某一行时，只考虑它前面token的作用。即：在计算 ![[公式]](https://www.zhihu.com/equation?tex=Z) 的第一行时，刻意地把 ![[公式]](https://www.zhihu.com/equation?tex=\text{Attention}) 矩阵第一行的后面几个元素屏蔽掉，只考虑 ![[公式]](https://www.zhihu.com/equation?tex=\text{Attention}_{0%2C0}) 。在产生have这个单词时，只考虑 I，不考虑之后的have a cat，即只会attend on已经产生的sequence，这个很合理，因为还没有产生出来的东西不存在，就无法做attention。



**Step3：**Masked Attention矩阵进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。得到的结果再与 ![[公式]](https://www.zhihu.com/equation?tex=V) 矩阵相乘得到最终的self-attention层的输出结果 ![[公式]](https://www.zhihu.com/equation?tex=Z_1\in+R_{N%2Cd}) 。



**Step4：** ![[公式]](https://www.zhihu.com/equation?tex=Z_1\in+R_{N%2Cd}) 只是某一个head的结果，将多个head的结果concat在一起之后再最后进行Linear Transformation得到最终的Masked Multi-Head Self-attention的输出结果 ![[公式]](https://www.zhihu.com/equation?tex=Z\in+R_{N%2Cd}) 。

<img src="https://pic4.zhimg.com/v2-b32b3c632a20f8daf12103dd05587fd7_b.jpg" alt="img" style="zoom: 67%;" />

图26：Masked Multi-Head Self-attention的具体操作



第1个**Masked Multi-Head Self-attention**的 ![[公式]](https://www.zhihu.com/equation?tex=\text{Query%2C+Key%2C+Value}) 均来自Output Embedding。

第2个**Multi-Head Self-attention**的 ![[公式]](https://www.zhihu.com/equation?tex=\text{Query}) 来自第1个Self-attention layer的输出， ![[公式]](https://www.zhihu.com/equation?tex=\text{Key%2C+Value}) 来自Encoder的输出。

![[公式]](https://www.zhihu.com/equation?tex=\text{Key%2C+Value}) 来自Transformer Encoder的输出，所以可以看做**句子(Sequence)/图片(image)**的**内容信息(content，比如句意是："我有一只猫"，图片内容是："有几辆车，几个人等等")**。

![[公式]](https://www.zhihu.com/equation?tex=\text{Query}) 表达了一种诉求：希望得到什么，可以看做**引导信息(guide)**。

通过Multi-Head Self-attention结合在一起的过程就相当于是**把我们需要的内容信息指导表达出来**。



Decoder的最后是Softmax 预测输出单词。因为 Mask 的存在，使得单词 0 的输出 ![[公式]](https://www.zhihu.com/equation?tex=Z(0%2C)) 只包含单词 0 的信息。Softmax 根据输出矩阵的每一行预测下一个单词，如下图27所示。

<img src="https://pic3.zhimg.com/v2-585526f8bfb9b4dfc691dfeb42562962_b.jpg" alt="img" style="zoom: 50%;" />

图27：Softmax 根据输出矩阵的每一行预测下一个单词



如下图28所示为Transformer的整体结构。

<img src="https://pic2.zhimg.com/v2-b9372cc3b3a810dba41e1a64d3b296d5_b.jpg" alt="img" style="zoom: 33%;" />

图28：Transformer的整体结构





## 3 Transformer+Detection：引入视觉领域的首创DETR

> **论文名称：End-to-End Object Detection with Transformers**

- **3.1 DETR原理分析：**

![[公式]](https://www.zhihu.com/equation?tex=\color{indianred}{\text{网络架构部分解读%3A}}) 

本文的任务是Object detection，用到的工具是Transformers，特点是End-to-end。

目标检测的任务是要去预测一系列的Bounding Box的坐标以及Label， 现代大多数检测器通过定义一些proposal，anchor或者windows，把问题构建成为一个分类和回归问题来间接地完成这个任务。**文章所做的工作，就是将transformers运用到了object detection领域，取代了现在的模型需要手工设计的工作，并且取得了不错的结果。**在object detection上DETR准确率和运行时间上和Faster RCNN相当；将模型 generalize 到 panoptic segmentation 任务上，DETR表现甚至还超过了其他的baseline。DETR第一个使用End to End的方式解决检测问题，解决的方法是把检测问题视作是一个set prediction problem，如下图29所示。

<img src="https://pic1.zhimg.com/v2-772984ccd82a0e0a279ea6a09c3c34c0_b.jpg" alt="img" style="zoom: 67%;" />

图29：DETR结合CNN和Transformer的结构，并行实现预测



网络的主要组成是CNN和Transformer，Transformer借助第1节讲到的self-attention机制，可以显式地对一个序列中的所有elements两两之间的interactions进行建模，使得这类transformer的结构非常适合带约束的set prediction的问题。DETR的特点是：一次预测，端到端训练，set loss function和二分匹配。

**文章的主要有两个关键的部分。**

**第一个是用transformer的encoder-decoder架构一次性生成** ![[公式]](https://www.zhihu.com/equation?tex=N) **个box prediction。其中** ![[公式]](https://www.zhihu.com/equation?tex=N) **是一个事先设定的、比远远大于image中object个数的一个整数。**

**第二个是设计了bipartite matching loss，基于预测的boxex和ground truth boxes的二分图匹配计算loss的大小，从而使得预测的box的位置和类别更接近于ground truth。**



DETR整体结构可以分为四个部分：backbone，encoder，decoder和FFN，如下图30所示，以下分别解释这四个部分：

<img src="https://pic4.zhimg.com/v2-3d43474df51c545ad6bafc19b3c8ccc3_b.jpg" alt="img" style="zoom: 67%;" />

图30：DETR整体结构



**1 首先看backbone：**CNN backbone处理 ![[公式]](https://www.zhihu.com/equation?tex=x_{\text{img}}\in+B\times+3\times+H_0+\times+W_0)维的图像，把它转换为![[公式]](https://www.zhihu.com/equation?tex=f\in+R^{B\times+C\times+H\times+W})维的feature map（一般来说 ![[公式]](https://www.zhihu.com/equation?tex=C+%3D+2048或256%2C+H+%3D+\frac{H_0}{32}%2C+W+%3D+\frac{W_0}{32})），backbone只做这一件事。



**2 再看encoder：**encoder的输入是![[公式]](https://www.zhihu.com/equation?tex=f\in+R^{B\times+C\times+H\times+W})维的feature map，接下来依次进行以下过程：



- **通道数压缩：**先用 ![[公式]](https://www.zhihu.com/equation?tex=1\times+1) convolution处理，将channels数量从 ![[公式]](https://www.zhihu.com/equation?tex=C) 压缩到 ![[公式]](https://www.zhihu.com/equation?tex=d)，即得到![[公式]](https://www.zhihu.com/equation?tex=z_0\in+R^{B\times+d\times+H\times+W})维的新feature map。
- **转化为序列化数据：**将空间的维度（高和宽）压缩为一个维度，即把上一步得到的![[公式]](https://www.zhihu.com/equation?tex=z_0\in+R^{B\times+d\times+H\times+W}(d%3D256))维的feature map通过reshape成![[公式]](https://www.zhihu.com/equation?tex=(HW%2CB%2C256))维的feature map。
- **位置编码：**在得到了![[公式]](https://www.zhihu.com/equation?tex=z_0\in+R^{B\times+d\times+H\times+W})维的feature map之后，正式输入encoder之前，需要进行**Positional Encoding**。这一步在第2节讲解transformer的时候已经提到过，因为**在self-attention中需要有表示位置的信息**，否则你的sequence = "A打了B" 还是sequence = "B打了A"的效果是一样的。**但是transformer encoder这个结构本身却无法体现出位置信息。**也就是说，我们需要对这个 ![[公式]](https://www.zhihu.com/equation?tex=z_0%5Cin+R%5E%7BB%5Ctimes+d%5Ctimes+H%5Ctimes+W%7D) 维的feature map做positional encoding。

进行完位置编码以后根据paper中的图片会有个相加的过程，如下图问号处所示。很多读者有疑问的地方是：论文图31示中相加的2个张量，一个是input embedding，另一个是位置编码维度看上去不一致，是怎么相加的？后面会解答。

<img src="https://pic1.zhimg.com/v2-a7e4de7ab9cc0d3015ca04cc251ee460_b.jpg" alt="img" style="zoom: 67%;" />

图31：怎么相加的？

原版Transformer和Vision Transformer (第4节讲述)的Positional Encoding的表达式为：

![[公式]](https://www.zhihu.com/equation?tex=\begin{align}PE_{(pos%2C+2i)}+%3D+sin(pos%2F10000^{2i%2Fd})+\\+PE_{(pos%2C+2i%2B1)}+%3D+cos(pos%2F10000^{2i%2Fd})++\end{align}\tag{10}) 

式中， ![[公式]](https://www.zhihu.com/equation?tex=d) 就是这个 ![[公式]](https://www.zhihu.com/equation?tex=d\times+HW) 维的feature map的第一维， ![[公式]](https://www.zhihu.com/equation?tex=pos\in+[1%2CHW]) 。表示token在sequence中的位置，sequence的长度是 ![[公式]](https://www.zhihu.com/equation?tex=HW) ，例如第一个token 的 ![[公式]](https://www.zhihu.com/equation?tex=pos%3D0) 。

![[公式]](https://www.zhihu.com/equation?tex=i) ，或者准确意义上是 ![[公式]](https://www.zhihu.com/equation?tex=2i) 和 ![[公式]](https://www.zhihu.com/equation?tex=2i%2B1) 表示了Positional Encoding的维度。所以当 ![[公式]](https://www.zhihu.com/equation?tex=pos) 为1时，对应的Positional Encoding可以写成：

![[公式]](https://www.zhihu.com/equation?tex=PE\left(+1+\right)%3D\left[+\sin+\left(+{1}%2F{{{10000}^{{0}%2F{256}\%3B}}}\%3B+\right)%2C\cos+\left(+{1}%2F{{{10000}^{{0}%2F{256}\%3B}}}\%3B+\right)%2C\sin+\left(+{1}%2F{{{10000}^{{2}%2F{256}\%3B}}}\%3B+\right)%2C\cos+\left(+{1}%2F{{{10000}^{{2}%2F{256}\%3B}}}\%3B+\right)%2C\ldots++\right])

式中， ![[公式]](https://www.zhihu.com/equation?tex={{d}_{}}%3D256)。

**第一点不同的是**，原版Transformer只考虑 ![[公式]](https://www.zhihu.com/equation?tex=x) 方向的位置编码，但是DETR考虑了 ![[公式]](https://www.zhihu.com/equation?tex=xy) 方向的位置编码，因为图像特征是2-D特征。采用的依然是 ![[公式]](https://www.zhihu.com/equation?tex=\text{sin+cos}) 模式，但是需要考虑 ![[公式]](https://www.zhihu.com/equation?tex=xy) 两个方向。不是类似vision transoformer做法简单的将其拉伸为 ![[公式]](https://www.zhihu.com/equation?tex=d\times+HW) ，然后从 ![[公式]](https://www.zhihu.com/equation?tex=[1%2CHW]) 进行长度为256的位置编码，而是考虑了 ![[公式]](https://www.zhihu.com/equation?tex=xy) 方向同时编码，每个方向各编码128维向量，这种编码方式更符合图像特点。

Positional Encoding的输出张量是： ![[公式]](https://www.zhihu.com/equation?tex=(B%2Cd%2CH%2CW)%2Cd%3D256) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=d) 代表位置编码的长度， ![[公式]](https://www.zhihu.com/equation?tex=H%2CW) 代表张量的位置。意思是说，这个特征图上的任意一个点 ![[公式]](https://www.zhihu.com/equation?tex=(H_1%2CW_1)) 有个位置编码，这个编码的长度是256，其中，前128维代表 ![[公式]](https://www.zhihu.com/equation?tex=H_1) 的位置编码，后128维代表 ![[公式]](https://www.zhihu.com/equation?tex=W_1) 的位置编码。

假设你想计算任意一个位置 ![[公式]](https://www.zhihu.com/equation?tex=(pos_x%2Cpos_y)%2Cpos_x\in+[1%2CHW]%2Cpos_y\in+[1%2CHW]) 的Positional Encoding，把 ![[公式]](https://www.zhihu.com/equation?tex=pos_x) 代入(11)式的 ![[公式]](https://www.zhihu.com/equation?tex=a) 式和 ![[公式]](https://www.zhihu.com/equation?tex=b) 式可以计算得到**128维的向量**，它代表 ![[公式]](https://www.zhihu.com/equation?tex=pos_x) 的位置编码，再把 ![[公式]](https://www.zhihu.com/equation?tex=pos_y) 代入(11)式的 ![[公式]](https://www.zhihu.com/equation?tex=c) 式和 ![[公式]](https://www.zhihu.com/equation?tex=d) 式可以计算得到**128维的向量**，它代表 ![[公式]](https://www.zhihu.com/equation?tex=pos_y) 的位置编码，把这2个128维的向量拼接起来，就得到了一个**256维的向量**，它代表 ![[公式]](https://www.zhihu.com/equation?tex=(pos_x%2Cpos_y)) 的位置编码。

计算所有位置的编码，就得到了 ![[公式]](https://www.zhihu.com/equation?tex=(256%2CH%2CW)) 的张量，代表这个batch的位置编码。编码矩阵的维度是 ![[公式]](https://www.zhihu.com/equation?tex=(B%2C256%2CH%2CW)) ，也把它**序列化成维度为** ![[公式]](https://www.zhihu.com/equation?tex=(HW%2CB%2C256)) 维的张量。

**准备与![[公式]](https://www.zhihu.com/equation?tex=(HW%2CB%2C256)) 维的feature map相加以后输入Encoder。**



值得注意的是，网上许多解读文章没有搞清楚 "转化为序列化数据"这一步和 "位置编码"的顺序关系，以及变量的shape到底是怎样变化的，这里我用一个图32表达，终结这个问题。

<img src="https://pic1.zhimg.com/v2-89d23b461169c6ab25ea64389fe8d86c_b.jpg" alt="img" style="zoom: 67%;" />

图32：变量的shape的变化，变量一律使用方块表达。

所以，了解了DETR的位置编码之后，你应该明白了其实input embedding和位置编码维度其实是一样的，只是论文图示为了突出二位编码所以画的不一样罢了，如下图33所示：

<img src="https://pic4.zhimg.com/v2-464b4196c273afbe67445d21b7bedc77_b.jpg" alt="img" style="zoom: 50%;" />

图33：input embedding与positional embedding的shape是一致的

**另一点不同的是，原版Transformer**只在Encoder之前使用了Positional Encoding，而且是**在输入上进行Positional Encoding，再把输入经过transformation matrix变为Query，Key和Value这几个张量。但是DETR**在Encoder的每一个Multi-head Self-attention之前都使用了Positional Encoding，且**只对Query和Key使用了Positional Encoding，即：只把维度为![[公式]](https://www.zhihu.com/equation?tex=(HW%2CB%2C256)) 维的位置编码与维度为![[公式]](https://www.zhihu.com/equation?tex=%28HW%2CB%2C256%29) 维的Query和Key相加，而不与Value相加。**

如图34所示为DETR的Transformer的详细结构，读者可以对比下原版Transformer的结构，如图21所示，为了阅读的方便我把图21又贴在下面了。

可以发现，除了Positional Encoding设置的不一样外，Encoder其他的结构是一致的。每个Encoder Layer包含一个multi-head self-attention 的module和一个前馈网络Feed Forward Network。

**Encoder最终输出的是 ![[公式]](https://www.zhihu.com/equation?tex=(H\cdot+W%2Cb%2C256)) 维的编码矩阵Embedding，按照原版Transformer的做法，把这个东西给Decoder。**

**总结下和原始transformer编码器不同的地方：**

- 输入编码器的位置编码需要考虑2-D空间位置。
- 位置编码向量需要加入到每个Encoder Layer中。
- 在编码器内部位置编码Positional Encoding仅仅作用于Query和Key，即只与Query和Key相加，Value不做任何处理。

<img src="https://pic3.zhimg.com/v2-c158521c7a602382dfa4d85243672df2_b.jpg" alt="img" style="zoom: 50%;" />

图34：Transformer详细结构。为了方便理解，我把每个变量的维度标在了图上。



<img src="https://pic4.zhimg.com/v2-1719966a223d98ad48f98c2e4d71add7_b.jpg" alt="img" style="zoom: 33%;" />

图21：Transformer整体结构




**3 再看decoder：**

DETR的Decoder和原版Transformer的decoder是不太一样的，如下图34和21所示。

先回忆下原版Transformer，看下图21的decoder的最后一个框：output probability，代表我们一次只产生一个单词的softmax，根据这个softmax得到这个单词的预测结果。这个过程我们表达为：**predicts the output sequence one element at a time**。

不同的是，DETR的Transformer Decoder是一次性处理全部的object queries，即一次性输出全部的predictions；而不像原始的Transformer是auto-regressive的，从左到右一个词一个词地输出。这个过程我们表达为：**decodes the N objects in parallel at each decoder layer。**

DETR的Decoder主要有两个输入：

1. **Transformer Encoder输出的Embedding与 position encoding 之和。**
2. **Object queries。**

其中，Embedding就是上文提到的 ![[公式]](https://www.zhihu.com/equation?tex=(H\cdot+W%2Cb%2C256)) 的编码矩阵。这里着重讲一下Object queries。

Object queries是一个维度为 ![[公式]](https://www.zhihu.com/equation?tex=(100%2Cb%2C256)) 维的张量，数值类型是nn.Embedding，说明这个张量是可以学习的，即：我们的Object queries是可学习的。Object queries矩阵内部通过学习建模了100个物体之间的全局关系，例如房间里面的桌子旁边(A类)一般是放椅子(B类)，而不会是放一头大象(C类)，那么在推理时候就可以利用该全局注意力更好的进行解码预测输出。

Decoder的输入一开始也初始化成维度为 ![[公式]](https://www.zhihu.com/equation?tex=(100%2Cb%2C256)) 维的全部元素都为0的张量，和Object queries加在一起之后**充当第1个multi-head self-attention的Query和Key。第一个multi-head self-attention的Value为Decoder的输入**，也就是全0的张量。

到了每个Decoder的第2个multi-head self-attention，它的Key和Value来自Encoder的输出张量，维度为 ![[公式]](https://www.zhihu.com/equation?tex=(hw%2Cb%2C256)) ，其中Key值还进行位置编码。Query值一部分来自第1个Add and Norm的输出，维度为 ![[公式]](https://www.zhihu.com/equation?tex=(100%2Cb%2C256)) 的张量，另一部分来自Object queries，充当可学习的位置编码。所以，第2个multi-head self-attention的Key和Value的维度为 ![[公式]](https://www.zhihu.com/equation?tex=%28hw%2Cb%2C256%29) ，而Query的维度为![[公式]](https://www.zhihu.com/equation?tex=%28100%2Cb%2C256%29)。

每个Decoder的输出维度为 ![[公式]](https://www.zhihu.com/equation?tex=(1%2Cb%2C100%2C256)) ，送入后面的前馈网络，具体的变量维度的变化见图30。

到这里你会发现：Object queries充当的其实是位置编码的作用，只不过它是可以学习的位置编码，所以，我们对Encoder和Decoder的每个self-attention的Query和Key的位置编码做个归纳，如图35所示，Value没有位置编码：

<img src="https://pic1.zhimg.com/v2-6b9de32f5e1174eb3ecfecc2f0335d48_b.jpg" alt="img" style="zoom: 50%;" />

图35：Transformer的位置编码来自哪里？



![[公式]](https://www.zhihu.com/equation?tex=\color{indianred}{\text{损失函数部分解读%3A}}) 

得到了Decoder的输出以后，如前文所述，应该是输出维度为 ![[公式]](https://www.zhihu.com/equation?tex=(b%2C100%2C256))的张量。接下来要送入2个前馈网络FFN得到class和Bounding Box。它们会得到 ![[公式]](https://www.zhihu.com/equation?tex=N%3D100) 个预测目标，包含类别和Bounding Box，当然这个100肯定是大于图中的目标总数的。如果不够100，则采用背景填充，计算loss时候回归分支分支仅仅计算有物体位置，背景集合忽略。所以，DETR输出张量的维度为输出的张量的维度是 ![[公式]](https://www.zhihu.com/equation?tex=(b%2C100%2C\color{crimson}{\text{class}%2B1})) 和 ![[公式]](https://www.zhihu.com/equation?tex=(b%2C100%2C\color{purple}{4}))。对应COCO数据集来说， ![[公式]](https://www.zhihu.com/equation?tex=\color{crimson}{\text{class}%2B1%3D92}) ， ![[公式]](https://www.zhihu.com/equation?tex=\color{purple}{4}) 指的是每个预测目标归一化的 ![[公式]](https://www.zhihu.com/equation?tex=(c_x%2Cc_y%2Cw%2Ch)) 。归一化就是除以图片宽高进行归一化。



到这里我们了解了DETR的网络架构，我们发现，它输出的张量的维度是 **分类分支：**![[公式]](https://www.zhihu.com/equation?tex=(b%2C100%2C\color{crimson}{\text{class}%2B1})) 和**回归分支：** ![[公式]](https://www.zhihu.com/equation?tex=(b%2C100%2C\color{purple}{4})) ，其中，前者是指100个预测框的类型，后者是指100个预测框的Bounding Box，但是读者可能会有疑问：预测框和真值是怎么一一对应的？换句话说：你怎么知道第47个预测框对应图片里的狗，第88个预测框对应图片里的车？等等。

我们下面就来聊聊这个问题。

相比Faster R-CNN等做法，DETR最大特点是将目标检测问题转化为无序集合预测问题(set prediction)。论文中特意指出Faster R-CNN这种设置一大堆anchor，然后基于anchor进行分类和回归其实属于代理做法即不是最直接做法，**目标检测任务就是输出无序集合**，而Faster R-CNN等算法通过各种操作，并结合复杂后处理最终才得到无序集合属于绕路了，而DETR就比较纯粹了。现在核心问题来了：输出的 ![[公式]](https://www.zhihu.com/equation?tex=(b%2C100)) 个检测结果是无序的，如何和 ![[公式]](https://www.zhihu.com/equation?tex=GT+\%3B+\text{Bounding+Box}) 计算loss？这就需要用到经典的双边匹配算法了，也就是常说的匈牙利算法，该算法广泛应用于最优分配问题。

一幅图片，我们把第 ![[公式]](https://www.zhihu.com/equation?tex=i) 个物体的真值表达为 ![[公式]](https://www.zhihu.com/equation?tex=y_i%3D(c_i%2Cb_i)) ，其中， ![[公式]](https://www.zhihu.com/equation?tex=c_i) 表示它的 ![[公式]](https://www.zhihu.com/equation?tex=\color{crimson}{\text{class}}) ， ![[公式]](https://www.zhihu.com/equation?tex=b_i) 表示它的 ![[公式]](https://www.zhihu.com/equation?tex=\color{purple}{\text{Bounding+Box}}) 。我们定义 ![[公式]](https://www.zhihu.com/equation?tex=\hat+y+%3D+\{\hat+y_i\}_{i%3D1}^{N}) 为网络输出的 ![[公式]](https://www.zhihu.com/equation?tex=N) 个预测值。

假设我们已经了解了什么是匈牙利算法(先假装了解了)，对于第 ![[公式]](https://www.zhihu.com/equation?tex=i) 个 ![[公式]](https://www.zhihu.com/equation?tex=GT) ， ![[公式]](https://www.zhihu.com/equation?tex=\sigma(i)) 为匈牙利算法得到的与 ![[公式]](https://www.zhihu.com/equation?tex=GT_i) 对应的prediction的索引。我举个栗子，比如 ![[公式]](https://www.zhihu.com/equation?tex=i%3D3%2C\sigma(i)%3D18) ，意思就是：与第3个真值对应的预测值是第18个。

那我能根据 ![[公式]](https://www.zhihu.com/equation?tex=\color{green}{\text{匈牙利算法}}) ，找到 ![[公式]](https://www.zhihu.com/equation?tex=\color{green}{\text{与每个真值对应的预测值是哪个}}) ，**那究竟是如何找到呢？**

![image-20211114222543854](https://i.loli.net/2021/11/14/RG3qWOtVAXhz2C1.png)

我们看看这个表达式是甚么意思，对于某一个真值 ![[公式]](https://www.zhihu.com/equation?tex=y_i) ，假设我们已经找到这个真值对应的预测值 ![[公式]](https://www.zhihu.com/equation?tex=\hat+y_{\sigma(i)}) ，这里的 ![[公式]](https://www.zhihu.com/equation?tex=\Sigma_N) 是所有可能的排列，代表**从真值索引到预测值索引的所有的映射**，然后用 ![[公式]](https://www.zhihu.com/equation?tex=L_{match}) 最小化 ![[公式]](https://www.zhihu.com/equation?tex=y_i) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat+y_%7B%5Csigma%28i%29%7D) 的距离。这个 ![[公式]](https://www.zhihu.com/equation?tex=L_%7Bmatch%7D) 具体是：

![image-20211114222514664](https://i.loli.net/2021/11/14/fNVdLb5rzp8W4OQ.png)

意思是：假设当前从真值索引到预测值索引的所有的映射为 ![[公式]](https://www.zhihu.com/equation?tex=\sigma) ，对于图片中的每个真值 ![[公式]](https://www.zhihu.com/equation?tex=i) ，先找到对应的预测值 ![[公式]](https://www.zhihu.com/equation?tex=\sigma(i)) ，再看看分类网络的结果 ![[公式]](https://www.zhihu.com/equation?tex=\hat+p_{\sigma(i)}(c_i)+) ，取反作为 ![[公式]](https://www.zhihu.com/equation?tex=L_%7Bmatch%7D) 的第1部分。再计算回归网络的结果 ![[公式]](https://www.zhihu.com/equation?tex=\hat+b_{\sigma(i)}) 与真值的 ![[公式]](https://www.zhihu.com/equation?tex=\color{purple}{\text{Bounding+Box}}) 的差异，即 ![[公式]](https://www.zhihu.com/equation?tex=L_{box}({b_{i}%2C+\hat+b_{\sigma(i)}})) ，作为 ![[公式]](https://www.zhihu.com/equation?tex=L_%7Bmatch%7D) 的第2部分。

所以，可以使得 ![[公式]](https://www.zhihu.com/equation?tex=L_%7Bmatch%7D) 最小的排列 ![[公式]](https://www.zhihu.com/equation?tex=\hat\sigma) 就是我们要找的排列，**即：对于图片中的每个真值 ![[公式]](https://www.zhihu.com/equation?tex=i) 来讲， ![[公式]](https://www.zhihu.com/equation?tex=\hat\sigma(i)) 就是这个真值所对应的预测值的索引。**



这就是匈牙利算法的过程。是不是与Anchor或Proposal有异曲同工的地方，只是此时我们找的是一对一匹配。

接下来就是使用上一步得到的排列 ![[公式]](https://www.zhihu.com/equation?tex=\hat\sigma) ，计算匈牙利损失：

![image-20211114221633495](https://i.loli.net/2021/11/14/fT3QKL8NpvYCsqa.png)

式中的 ![[公式]](https://www.zhihu.com/equation?tex=L_{box}) 具体为：

![image-20211114221643443](https://i.loli.net/2021/11/14/8wOEsQyhDHmaMGT.png)

最常用的 ![[公式]](https://www.zhihu.com/equation?tex=L_1+\%3Bloss) 对于大小 ![[公式]](https://www.zhihu.com/equation?tex=\color{purple}{\text{Bounding+Box}}) 会有不同的标度，即使它们的相对误差是相似的。为了缓解这个问题，作者使用了 ![[公式]](https://www.zhihu.com/equation?tex=L_1+%5C%3Bloss) 和广义IoU损耗 ![[公式]](https://www.zhihu.com/equation?tex=L_{iou}) 的线性组合，它是比例不变的。

Hungarian意思就是匈牙利，也就是前面的 ![[公式]](https://www.zhihu.com/equation?tex=L_%7Bmatch%7D) ，上述意思是需要计算 ![[公式]](https://www.zhihu.com/equation?tex=M) 个 ![[公式]](https://www.zhihu.com/equation?tex=\text{GT}\%3B\color{purple}{\text{Bounding+Box}}) 和 ![[公式]](https://www.zhihu.com/equation?tex=N) 个输预测出集合两两之间的广义距离，**距离越近表示越可能是最优匹配关系**，也就是两者最密切。广义距离的计算考虑了分类分支和回归分支。



**DETR是怎么训练的？**

训练集里面的任何一张图片，假设第1张图片，我们通过模型产生100个预测框 ![[公式]](https://www.zhihu.com/equation?tex=\text{Predict}\%3B\color{purple}{\text{Bounding+Box}}) ，假设这张图片有只3个 ![[公式]](https://www.zhihu.com/equation?tex=\text{GT}\%3B\color{purple}{\text{Bounding+Box}}) ，它们分别是 ![[公式]](https://www.zhihu.com/equation?tex=\color{orange}{\text{Car}}%2C\color{green}{\text{Dog}}%2C\color{darkturquoise}{\text{Horse}}) 。 

![[公式]](https://www.zhihu.com/equation?tex=(\text{label}_{\color{orange}{\text{Car}}}%3D3%2C\text{label}_{\color{green}{\text{Dog}}}%3D24%2C\text{label}_{\color{orange}{\color{darkturquoise}{\text{Horse}}}}%3D75)\\) 

问题是：我怎么知道这100个预测框哪个是对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{orange}{\text{Car}}) ，哪个是对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{green}{\text{Dog}}) ，哪个是对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{darkturquoise}{\text{Horse}}) ？

我们建立一个 ![[公式]](https://www.zhihu.com/equation?tex=(100%2C3)) 的矩阵，矩阵里面的元素就是 ![[公式]](https://www.zhihu.com/equation?tex=(13)) 式的计算结果，举个例子：比如左上角的 ![[公式]](https://www.zhihu.com/equation?tex=(1%2C1)) 号元素的含义是：第1个预测框对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{orange}{\text{Car}}(\text{label}%3D3)) 的情况下的 ![[公式]](https://www.zhihu.com/equation?tex=L_%7Bmatch%7D) 值。我们用**scipy.optimize** 这个库中的 **linear_sum_assignment** 函数找到最优的匹配，这个过程我们称之为：**"匈牙利算法 (Hungarian Algorithm)"**。

假设**linear_sum_assignment**做完以后的结果是：第 ![[公式]](https://www.zhihu.com/equation?tex=23) 个预测框对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{orange}{\text{Car}}) ，第 ![[公式]](https://www.zhihu.com/equation?tex=44) 个预测框对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{green}{\text{Dog}}) ，第 ![[公式]](https://www.zhihu.com/equation?tex=95) 个预测框对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{darkturquoise}{\text{Horse}}) 。

现在把第 ![[公式]](https://www.zhihu.com/equation?tex=23%2C44%2C95) 个预测框挑出来，按照 ![[公式]](https://www.zhihu.com/equation?tex=(14)) 式计算Loss，得到这个图片的Loss。

把所有的图片按照这个模式去训练模型。

**训练完以后怎么用？**

训练完以后，你的模型学习到了一种能力，即：模型产生的100个预测框，它知道某个预测框该对应什么 ![[公式]](https://www.zhihu.com/equation?tex=\text{Object}) ，比如，模型学习到：第1个 ![[公式]](https://www.zhihu.com/equation?tex=\text{Predict}\%3B\color{purple}{\text{Bounding+Box}}) 对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{orange}{\text{Car}}(\text{label}%3D3)) ，第2个 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BPredict%7D%5C%3B%5Ccolor%7Bpurple%7D%7B%5Ctext%7BBounding+Box%7D%7D) 对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{chocolate}{\text{Bus}}(\text{label}%3D16)) ，第3个 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BPredict%7D%5C%3B%5Ccolor%7Bpurple%7D%7B%5Ctext%7BBounding+Box%7D%7D) 对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{lightskyblue}{\text{Sky}}(\text{label}%3D21)) ，第4个 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BPredict%7D%5C%3B%5Ccolor%7Bpurple%7D%7B%5Ctext%7BBounding+Box%7D%7D) 对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{green}{\text{Dog}}(\text{label}%3D24)) ，第5个 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BPredict%7D%5C%3B%5Ccolor%7Bpurple%7D%7B%5Ctext%7BBounding+Box%7D%7D) 对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{darkturquoise}{\text{Horse}}(\text{label}%3D75)) ，第6-100个 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BPredict%7D%5C%3B%5Ccolor%7Bpurple%7D%7B%5Ctext%7BBounding+Box%7D%7D) 对应 ![[公式]](https://www.zhihu.com/equation?tex=\color{dimgray}{\varnothing+}(\text{label}%3D92)) ，等等。

以上只是我举的一个例子，意思是说：模型知道了自己的100个预测框每个该做什么事情，即：每个框该预测什么样的 ![[公式]](https://www.zhihu.com/equation?tex=\text{Object}) 。

**为什么训练完以后，模型学习到了一种能力，即：模型产生的100个预测框，它知道某个预测框该对应什么 ![[公式]](https://www.zhihu.com/equation?tex=\text{Object}) ？**

还记得前面说的Object queries吗？它是一个维度为 ![[公式]](https://www.zhihu.com/equation?tex=%28100%2Cb%2C256%29) 维的张量，初始时元素全为 ![[公式]](https://www.zhihu.com/equation?tex=0) 。实现方式是**nn.Embedding(num_queries, hidden_dim)**，这里num_queries=100，hidden_dim=256，它是可训练的。这里的 ![[公式]](https://www.zhihu.com/equation?tex=b) 指的是batch size，我们考虑单张图片，所以假设Object queries是一个维度为 ![[公式]](https://www.zhihu.com/equation?tex=(100%2C256)) 维的张量。我们训练完模型以后，这个张量已经训练完了，那**此时的Object queries究竟代表什么？**

我们把此时的Object queries**看成100个格子，每个格子是个256维的向量。**训练完以后，这100个格子里面**注入了不同 ![[公式]](https://www.zhihu.com/equation?tex=\text{Object}) 的位置信息和类别信息**。**比如第1个格子里面的这个256维的向量代表着 ![[公式]](https://www.zhihu.com/equation?tex=\color{orange}{\text{Car}}) 这种 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BObject%7D) 的位置信息，**这种信息是通过训练，考虑了所有图片的某个位置附近的 ![[公式]](https://www.zhihu.com/equation?tex=%5Ccolor%7Borange%7D%7B%5Ctext%7BCar%7D%7D) 编码特征，属于和位置有关的全局 ![[公式]](https://www.zhihu.com/equation?tex=%5Ccolor%7Borange%7D%7B%5Ctext%7BCar%7D%7D) 统计信息。

测试时，假设图片中有 ![[公式]](https://www.zhihu.com/equation?tex=\color{orange}{\text{Car}}%2C\color{green}{\text{Dog}}%2C\color{darkturquoise}{\text{Horse}}) 三种物体，该图片会输入到编码器中进行特征编码，假设特征没有丢失，Decoder的**Key**和**Value**就是编码器输出的编码向量(如图30所示)，而Query就是Object queries，就是我们的100个格子。

**Query可以视作代表不同 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BObject%7D) 的信息，而Key和Value可以视作代表图像的全局信息。**

现在通过注意力模块将**Query**和**Key**计算，然后加权**Value**得到解码器输出。对于第1个格子的**Query**会和**Key**中的所有向量进行计算，目的是查找某个位置附近有没有 ![[公式]](https://www.zhihu.com/equation?tex=%5Ccolor%7Borange%7D%7B%5Ctext%7BCar%7D%7D) ，如果有那么该特征就会加权输出，对于第3个格子的**Query**会和**Key**中的所有向量进行计算，目的是查找某个位置附近有没有 ![[公式]](https://www.zhihu.com/equation?tex=\color{lightskyblue}{\text{Sky}}) ，很遗憾，这个没有，所以输出的信息里面没有 ![[公式]](https://www.zhihu.com/equation?tex=%5Ccolor%7Blightskyblue%7D%7B%5Ctext%7BSky%7D%7D) 。

整个过程计算完成后就可以把编码向量中的 ![[公式]](https://www.zhihu.com/equation?tex=\color{orange}{\text{Car}}%2C\color{green}{\text{Dog}}%2C\color{darkturquoise}{\text{Horse}}) 的编码嵌入信息提取出来，然后后面接 ![[公式]](https://www.zhihu.com/equation?tex=FFN) 进行分类和回归就比较容易，因为特征已经对齐了。



发现了吗？Object queries在训练过程中对于 ![[公式]](https://www.zhihu.com/equation?tex=N) 个格子会压缩入对应的和位置和类别相关的统计信息，在测试阶段就可以利用该**Query**去和**某个图像的编码特征Key，Value**计算，**若图片中刚好有Query想找的特征，比如** ![[公式]](https://www.zhihu.com/equation?tex=%5Ccolor%7Borange%7D%7B%5Ctext%7BCar%7D%7D) **，则这个特征就能提取出来，最后通过2个** ![[公式]](https://www.zhihu.com/equation?tex=FFN) **进行分类和回归。**所以前面才会说Object queries作用非常类似Faster R-CNN中的anchor，这个anchor是可学习的，由于维度比较高，故可以表征的东西丰富，当然维度越高，训练时长就会越长。

**这就是DETR的End-to-End的原理，可以简单归结为上面的几段话，你读懂了上面的话，也就明白了DETR以及End-to-End的Detection模型原理。**



**Experiments：**

**1. 性能对比：**

<img src="https://pic4.zhimg.com/v2-a82446719e7ebd58ac2b3680bd096b6b_b.jpg" alt="img" style="zoom: 50%;" />

图36：DETR与Fast R-CNN的性能对比

**2. 编码器层数对比实验：**

<img src="https://pic2.zhimg.com/v2-35d68162e148aa1457e7f91d135cfdf1_b.jpg" alt="img" style="zoom: 50%;" />

图37：编码器数目与模型性能

可以发现，编码器层数越多越好，最后就选择6。

下图38为最后一个Encoder Layer的attention可视化，Encoder已经分离了instances，简化了Decoder的对象提取和定位。

<img src="https://pic3.zhimg.com/v2-dffe148c6e78f7b67cf6aa5c8bbbc316_b.jpg" alt="img" style="zoom: 67%;" />

图38：最后一个Encoder Layer的attention可视化



**3. 解码器层数对比实验：**

<img src="https://pic4.zhimg.com/v2-87f7c11d6b088af0d0351e3e4808e4b7_b.jpg" alt="img" style="zoom: 67%;" />

图39：每个Decoder Layer后的AP和AP 50性能。

可以发现，性能随着解码器层数的增加而提升，DETR本不需要NMS，但是作者也进行了，上图中的NMS操作是指DETR的每个解码层都可以输入无序集合，那么将所有解码器无序集合全部保留，然后进行NMS得到最终输出，可以发现性能稍微有提升，特别是AP50。这可以通过以下事实来解释：Transformer的单个Decoder Layer不能计算输出元素之间的任何互相关，因此它易于对同一对象进行多次预测。在第2个和随后的Decoder Layer中，self-attention允许模型抑制重复预测。所以NMS带来的改善随着Decoder Layer的增加而减少。在最后几层，作者观察到AP的一个小损失，因为NMS错误地删除了真实的positive prediction。

<img src="https://pic1.zhimg.com/v2-6b80634bf88e496f035945ec25c40764_b.jpg" alt="img" style="zoom: 67%;" />

图40： Decoder Layer的attention可视化

类似于可视化编码器注意力，作者在图40中可视化解码器注意力，用不同的颜色给每个预测对象的注意力图着色。观察到，解码器的attention相当局部，这意味着它主要关注对象的四肢，如头部或腿部。我们假设，在编码器通过全局关注分离实例之后，**解码器只需要关注极端来提取类和对象边界。**



## 4 参考文献

[1] Zhu, Xizhou et al. “Deformable DETR: Deformable Transformers for End-to-End Object Detection.” *ArXiv* abs/2010.04159 (2021): n. pag.

[2] Carion, Nicolas et al. “End-to-End Object Detection with Transformers.” *ArXiv* abs/2005.12872 (2020): n. pag.

[3] Jaderberg, Max et al. “Spatial Transformer Networks.” *NIPS* (2015).

[4] Vaswani, Ashish et al. “Attention is All you Need.” *ArXiv* abs/1706.03762 (2017): n. pag.

[5] Devlin, Jacob et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” *NAACL* (2019).

[6] Yuan, Li et al. “Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.” *ArXiv* abs/2101.11986 (2021): n. pag.

[7] Wang, Wenhai et al. “Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions.” *ArXiv* abs/2102.12122 (2021): n. pag.

[8] Zhao, Hengshuang et al. “Exploring Self-Attention for Image Recognition.” *2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)* (2020): 10073-10082.

[9] Shi, Xingjian et al. “Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting.” *NIPS* (2015).

[10] Greff, Klaus et al. “LSTM: A Search Space Odyssey.” *IEEE Transactions on Neural Networks and Learning Systems* 28 (2017): 2222-2232.

[11] Cho, Kyunghyun et al. “Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation.” *EMNLP* (2014).

[12] Hochreiter, Sepp and Jürgen Schmidhuber. “Long Short-Term Memory.” *Neural Computation* 9 (1997): 1735-1780.



